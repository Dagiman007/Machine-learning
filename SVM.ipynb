{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "61IkqvVdGLah"
   },
   "source": [
    "# Machine Learning Homework 3\n",
    "\n",
    "Student: Dagmawi Abraham Seifu\n",
    "\n",
    "Professor: Pasquale Caianiello\n",
    "\n",
    "# Homework problem: \n",
    "Support Vector Machine (SVM) algorithm implementation for Generalized learning to predict any of specified features from a dataset.\n",
    "\n",
    "I used SVC module from sklearn to run SVM algorithm on encoded dataset, in addition 'pipeline' class from scikit module is used to combine multiple processes as a single estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qzdaV-jkHSxe"
   },
   "source": [
    "# Dataset\n",
    "For this homework I used the Letter recognition dataset, which contains 20000 samples with 16 different type of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aeVn1Yu6sZg0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import random, copy\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from sklearn.externals import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YVrM9dHGxq4I"
   },
   "source": [
    "We first create a CharacteristicVector class that has two functions for encoding and decoding columns (features) of a given dataset. The unique values in a column determine how many bits are going to be used in encoding the feature. If, for instance, a column contains 10 unique values, then those values are taken as classes of that feature and are represented with 10 bits (1000000000, 0100000000, ...). If a value in a column is fraction(float), it will be rounded to the closest integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4zDhNsIvsl3t"
   },
   "outputs": [],
   "source": [
    "class CharacteristicVector:\n",
    "  def __init__(self):\n",
    "    self.columnMap = np.array([])\n",
    "\n",
    "  def encode(self, column):\n",
    "    column = column.astype(str) if not str(column.dtypes).startswith(\"float\") else column.apply(np.round).astype(int).astype(str)\n",
    "\n",
    "    uniqueColumn = np.unique(column.as_matrix())\n",
    "    for i in range(0, len(uniqueColumn)):\n",
    "      self.columnMap = np.append(self.columnMap, uniqueColumn[i])\n",
    "\n",
    "    characteristicVector = []\n",
    "\n",
    "    for i in range(0, len(column)):\n",
    "      value = column[i]\n",
    "      binary = np.zeros(len(self.columnMap), dtype=np.int)\n",
    "      binary[self.columnMap.tolist().index(value)] = 1\n",
    "      characteristicVector += [binary]\n",
    "\n",
    "    return np.array(characteristicVector)\n",
    "\n",
    "  def decode(self, column):\n",
    "    vector = []\n",
    "\n",
    "    for i in range(0, len(column)):\n",
    "      binary = column[i]\n",
    "      index = binary.tolist().index(1)\n",
    "      value = self.columnMap[index]\n",
    "      vector += [value] \n",
    "\n",
    "    return np.array(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HOcaOlwIs1PC"
   },
   "outputs": [],
   "source": [
    "ltr = 'Data/Letter recognition/letter-recognition.data'\n",
    "dataSet = pd.read_csv(ltr, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mfmykRwfokgc"
   },
   "source": [
    "We split the dataset randomly into training and test set, with 80/20 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "flQUFf18uoLj"
   },
   "outputs": [],
   "source": [
    "indexes = np.arange(len(dataSet)).tolist()\n",
    "indexesTrainingSet = random.sample(indexes, int(0.8*len(dataSet)))\n",
    "trainingSet = dataSet.iloc[indexesTrainingSet]\n",
    "testSet = dataSet.iloc[list(set(indexes) - set(indexesTrainingSet))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b3ZZqVoTo0kn"
   },
   "source": [
    "Write the training set and the test set in separate files, in order to make encoding and decoding much easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zDMdlAYXuvQJ"
   },
   "outputs": [],
   "source": [
    "training_file = open('trainingSet.csv', \"w+\")\n",
    "writer = csv.writer(training_file, delimiter=',')\n",
    "writer.writerows(trainingSet.values)\n",
    "\n",
    "test_file = open('testSet.csv', \"w+\")\n",
    "writer = csv.writer(test_file, delimiter=',')\n",
    "writer.writerows(testSet.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_yMIjRtjqFdq"
   },
   "source": [
    "Encode the dataset using the using the CharacteristicVector class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "4nR-fubuu74n",
    "outputId": "384ebb1a-2404-42e1-d844-b080a5525768"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "cv = CharacteristicVector()\n",
    "characteristicVectorLabel = cv.encode(dataSet[0])  # encode the label\n",
    "lengthCharacteristicVectorLabel = len(cv.columnMap)\n",
    "cvFeatures = []\n",
    "characteristicVectorFeatures = []\n",
    "startPosition = lengthCharacteristicVectorLabel\n",
    "\n",
    "for i in range(1, len(dataSet.columns)) :\n",
    "  cvFeature = CharacteristicVector()\n",
    "  columnDataSet = dataSet[i]\n",
    "  characteristicVector = cvFeature.encode(columnDataSet)\n",
    "  lengthCharacteristicVector = len(cvFeature.columnMap)\n",
    "\n",
    "  cvFeatures += [[cvFeature, lengthCharacteristicVector, startPosition]]\n",
    "\n",
    "  startPosition += lengthCharacteristicVector\n",
    "\n",
    "  characteristicVectorFeatures += [characteristicVector]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "glc-c7peVmHZ"
   },
   "source": [
    "Write the encoded labels and the remaining features into separate files for easy access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RHIybwdivOKg",
    "outputId": "5065117f-bb3a-41ce-83f6-87f6601b16a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cvFeatures.pkl']"
      ]
     },
     "execution_count": 75,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(cv, 'cvLabel.pkl') \n",
    "joblib.dump(cvFeatures, 'cvFeatures.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pu7cb7qMrBwP"
   },
   "source": [
    "Write the encoded dataset in another file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wv59VsokvRzW"
   },
   "outputs": [],
   "source": [
    "file = open('dataSetEncoded.csv', \"w+\")\n",
    "for i in range(0, len(dataSet)) :\n",
    "  label = characteristicVectorLabel[i]\n",
    "  line = label\n",
    "  for j in range(0, len(characteristicVectorFeatures)):\n",
    "    column = characteristicVectorFeatures[j]\n",
    "    line = np.concatenate((line, column[i]))\n",
    "  file.write(','.join(map(str, line.tolist()))+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X-B7vIOUWDTt"
   },
   "source": [
    "Open the encoded dataset and split it into training set and test set with ratio 80/20 as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nPSbPqUzvf4r"
   },
   "outputs": [],
   "source": [
    "dataSetEncoded = pd.read_csv(\"dataSetEncoded.csv\", header=None)\n",
    "trainingSetEncoded = dataSetEncoded.iloc[indexesTrainingSet]\n",
    "testSetEncoded = dataSetEncoded.iloc[list(set(indexes) - set(indexesTrainingSet))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SP2DP_QNwUQb",
    "outputId": "25d7a692-4808-4e11-af49-cbcdf7aca3ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pipeline.pkl']"
      ]
     },
     "execution_count": 78,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = trainingSetEncoded.drop(np.arange(len(trainingSetEncoded.columns)/2, len(trainingSetEncoded.columns)), axis=1)\n",
    "y = trainingSetEncoded[np.arange(len(trainingSetEncoded.columns)/2, len(trainingSetEncoded.columns))]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "#scaler.fit(X)\n",
    "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "svm = SVC(kernel = 'poly', degree = 8)\n",
    "steps = [('scaler', StandardScaler()), ('svm', SVC())]\n",
    "  \n",
    "pipeline = Pipeline(steps)\n",
    "pipeline.fit(X,y.iloc[:,0])\n",
    "\n",
    "#X_train = scaler.transform(X)\n",
    "\n",
    "  \n",
    "joblib.dump(pipeline, 'pipeline.pkl') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hlDqg_oTw7j2",
    "outputId": "7b76cbc6-289b-49d3-c04b-49576455f80e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Encoded Test: 99.4%\n"
     ]
    }
   ],
   "source": [
    "X_test = testSetEncoded.drop(np.arange(len(testSetEncoded.columns)/2, len(testSetEncoded.columns)), axis=1)\n",
    "y_test = testSetEncoded[np.arange(len(testSetEncoded.columns)/2, len(testSetEncoded.columns))]\n",
    "\n",
    "print(\"Accuracy for Encoded Test: \" + str(pipeline.score(X_test,y_test.iloc[:,0])*100) + \"%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3IyoJK9LjMrK"
   },
   "source": [
    "Now let us implement the support vector machine algorithm for the original dataset (the unencoded one) to compare the prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "gRFPc44Zg6DL",
    "outputId": "14e05c6f-8582-4152-97f5-68d29cdb0652"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.45\n"
     ]
    }
   ],
   "source": [
    "X_train = trainingSet.iloc[:,1:17]\n",
    "y_train = trainingSet.iloc[:,0]\n",
    "X_test = testSet.iloc[:,1:17]\n",
    "y_test = testSet.iloc[:,0]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scale = scaler.transform(X_train)\n",
    "X_test_scale = scaler.transform(X_test) \n",
    "svm = SVC(kernel = 'poly', degree=8)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "print(\"Accuracy:\",accuracy_score(y_test, y_pred)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bbXXwvUFjLBV"
   },
   "source": [
    "As we can see, SVM will give much higher almost perfect prediction accuracy on the encoded dataset than the original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A1FS4OMQH-Ob"
   },
   "source": [
    "# Reference\n",
    "https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SVM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
